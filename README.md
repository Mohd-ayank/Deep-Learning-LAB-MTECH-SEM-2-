# Deep-Learning-LAB-MTECH-SEM-2-
------------------------------------------
****experiment-1:
Create 1D, 2D, and 3D tensors using PyTorch and NumPy.
Show Basic Operations: Element-wise addition, subtraction, multiplication, and division.
Dot product and matrix multiplication.
Indexing and Slicing examples (Boolean masking, extracting subtensors)
Use .view(), .reshape(), .unsqueeze(), and .squeeze() in PyTorch. Compare with .reshape in Numpy
Broadcasting- Perform operations with tensors of different shapes.
In-place vs Out-of-place operations**
**

**Experiment 2 : Neural Network from Scratch using Numpy on MNIST dataset**

**Experiment-3: 1.perceptron for linearly separable data.
2. MLP for non-linearly separable data**

**Experiment-4: The objective of this lab is to implement Convolutional Neural Networks (CNNs) to classify
images in the Cats vs. Dogs dataset and the CIFAR-10 dataset. You will explore different
configurations by experimenting with:
● 3 Activation Functions
● 3 Weight Initialization Techniques
● 3 Optimizers
Additionally, you will compare your best CNN model for both datasets with a pretrained
ResNet-18 model.**

****Experiment-5: The aim of this experiment is to explore text generation using Recurrent Neural
Networks (RNNs) and to understand the impact of different word representations:
1. One-Hot Encoding
2. Trainable Word Embeddings
As part of the experiment, a basic RNN is first implemented from scratch
using NumPy to understand its internal workings. The RNN model is then trained
using PyTorch on a dataset of 100 poems, and the performance of both encoding
techniques is compared.****
