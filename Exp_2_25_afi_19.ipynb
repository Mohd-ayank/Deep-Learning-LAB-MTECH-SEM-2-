{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKJoj_tKDKZi"
      },
      "outputs": [],
      "source": [
        "#experiment-2 -> building a neural network using numpy\n",
        "'''-> USING MNIST DATASET\n",
        "-> make a repo in github and put all experimnets inside that\n",
        "-> make sure proper comments in the code\n",
        "-> submit github link\n",
        "-> connect directly colab with github\n",
        "-> update README file experimentwise'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NOTE -> I am using tensorflow just to load MNIST dataset\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "36FTAM2w_NYJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MATH FUNCTION UTILITIES**"
      ],
      "metadata": {
        "id": "bsE4Fu0G_3i6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sigmoid activation function\n",
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))\n",
        "\n",
        "#derivative used for back propagation\n",
        "def sigmoid_derivative(z):\n",
        "  return z*(1-z)\n",
        "\n",
        "# def softmax(z):\n",
        "#     exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # stability\n",
        "#     return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "#Converts labels (e.g., 3) into vectors\n",
        "def one_hot_encode(labels, num_classes=10):\n",
        "    output = np.zeros((len(labels), num_classes))\n",
        "    for i, label in enumerate(labels):\n",
        "        output[i][label] = 1\n",
        "    return output\n",
        "\n",
        "\n",
        "#limiting our dataset to 1000 only\n",
        "def load_data(limit=1000):\n",
        "\n",
        "    \"\"\"Loads MNIST, normalizes inputs, and encodes labels.\"\"\"\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "    # taking only initial 1000 samples for faster training\n",
        "    X = X_train[:limit]\n",
        "    y = y_train[:limit]\n",
        "\n",
        "    # Flatten images (28x28 -> 784) and Normalize (0-255 -> 0-1) for better convergence\n",
        "    X_flat = X.reshape(X.shape[0], 784) / 255.0\n",
        "\n",
        "    # One hot encode y\n",
        "    y_encoded = one_hot_encode(y)\n",
        "\n",
        "    return X_flat, y_encoded, y # Return raw y for accuracy checking\n",
        "\n",
        "\n",
        "# Load the data once\n",
        "X, y_encoded, y = load_data(1000)\n",
        "print(f\"Data Loaded: X shape: {X.shape}, y shape: {y_encoded.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ2N0Y1G_rag",
        "outputId": "c2717b39-5a31-4583-f047-b25cc79712dc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Loaded: X shape: (1000, 784), y shape: (1000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NETWORK TRAINING WITH 1 HIDDEN LAYER ONLY**"
      ],
      "metadata": {
        "id": "LGcu6FIsFOBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#you could definitely use He or xavier initialization based on activation function you using\n",
        "def init_1_layer(input_size, hidden_size, output_size):\n",
        "    np.random.seed(40)#for same reproducibility\n",
        "    weights = {\n",
        "        'w1': 2 * np.random.random((input_size, hidden_size)) - 1,\n",
        "        'w2': 2 * np.random.random((hidden_size, output_size)) - 1\n",
        "    }\n",
        "    return weights\n",
        "'''\n",
        "np.random.random()       → [0, 1)\n",
        "2 * random               → [0, 2)\n",
        "2 * random - 1           → [-1, 1)\n",
        "\n",
        "Centered Around Zero(Mean ≈ 0)\n",
        "Positive and negative weights. This is important because:\n",
        "Activations stay balanced\n",
        "Gradients don’t explode immediately'''\n",
        "\n",
        "def forward_1_layer(X, weights):\n",
        "    z1 = np.dot(X, weights['w1'])\n",
        "    a1 = sigmoid(z1)\n",
        "\n",
        "    z2 = np.dot(a1, weights['w2'])\n",
        "    output = sigmoid(z2)#softmax for multiclass classification\n",
        "\n",
        "    return a1, output\n",
        "\n",
        "\n",
        "def train_1_layer(X, y, epochs=1000, lr=0.005):\n",
        "    print(\"\\n**** Training 1 Hidden Layer ANN ******\")\n",
        "    weights = init_1_layer(784, 64, 10)\n",
        "\n",
        "    for i in range(epochs):\n",
        "        # 1. Forward\n",
        "        a1, output = forward_1_layer(X, weights)\n",
        "\n",
        "        # 2. Backward (Calculate Errors)\n",
        "        output_error = y - output\n",
        "        output_delta = output_error * sigmoid_derivative(output)\n",
        "        # #categorical cross entropy\n",
        "        # output_delta = output - y\n",
        "\n",
        "        a1_error = output_delta.dot(weights['w2'].T)#transpose\n",
        "        a1_delta = a1_error * sigmoid_derivative(a1)\n",
        "\n",
        "        # 3. Update Weights\n",
        "        weights['w2'] += lr * a1.T.dot(output_delta)\n",
        "        weights['w1'] += lr * X.T.dot(a1_delta)\n",
        "\n",
        "        # Monitor Progress\n",
        "        if (i % 200 == 0 or i==999):\n",
        "            acc = np.mean(np.argmax(output, axis=1) == np.argmax(y, axis=1))\n",
        "            print(f\"Epoch {i}: Accuracy = {acc * 100:.2f}%\")\n",
        "\n",
        "    return weights\n",
        "\n",
        "# Run it\n",
        "final_weights_1 = train_1_layer(X, y_encoded, epochs=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHe9cWoHFLhy",
        "outputId": "1d1bc6ea-562a-4f31-98b3-6d0e887833a9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**** Training 1 Hidden Layer ANN ******\n",
            "Epoch 0: Accuracy = 9.20%\n",
            "Epoch 200: Accuracy = 62.40%\n",
            "Epoch 400: Accuracy = 77.00%\n",
            "Epoch 600: Accuracy = 94.80%\n",
            "Epoch 800: Accuracy = 96.90%\n",
            "Epoch 999: Accuracy = 98.00%\n"
          ]
        }
      ]
    }
  ]
}